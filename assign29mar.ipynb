{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f27d5072-28ff-49a9-a330-d99e41ed0fca",
   "metadata": {},
   "source": [
    "## Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0aa263-2aac-4ff7-ad73-e770c46548b6",
   "metadata": {},
   "source": [
    "## In statistics and machine learning, lasso (least absolute shrinkage and selection operator; also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the resulting statistical model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527cc9ec-5524-4910-80ea-0b420e07c1c0",
   "metadata": {},
   "source": [
    "\n",
    "## This is a regularization technique used in feature selection using a Shrinkage method also referred to as the penalized regression method. Lasso is short for Least Absolute Shrinkage and Selection Operator, which is used both for regularization and model selection.\n",
    "## Ridge Regression adds a penalty term proportional to the square of the coefficients, while Lasso adds a penalty term proportional to the absolute value of the coefficients, which can lead to variable selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a0a32d-893c-4638-841e-9817602071eb",
   "metadata": {},
   "source": [
    "## Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea1c75b-b7fa-4816-9449-a3c1f67d58d4",
   "metadata": {},
   "source": [
    "## Advantages of LASSO Regression: LASSO involves a penalty factor that determines how many features are retained; using cross-validation to choose the penalty factor helps assure that the model will generalize well to future data samples.\n",
    "## Lasso regression is a valuable technique for feature selection and regularization in linear regression models. By penalizing the sum of absolute coefficients, it effectively shrinks less important features to zero, improving model interpretability and reducing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d0a7c9-9c5e-4c6a-86e9-c7a7facf73fc",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcf7ac6-ccdc-4a37-9e7c-16b374059e6d",
   "metadata": {},
   "source": [
    "## Lasso regression and force coefficients toward 0. The smaller the coefficient the less important it is or less variance it explains. The actual value here will be less important since it will be used in logistic regression because it will end up being used in an exponential.\n",
    "## The lasso coefficients become zero in a certain range and are reduced by a constant factor, which explains their low magnitude in comparison to the ridge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1cea3e-818c-4be0-be0e-53b0e8c0628f",
   "metadata": {},
   "source": [
    "## Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c530517-b43d-4a22-b035-6a44e4f2084e",
   "metadata": {},
   "source": [
    "## A tuning parameter (λ), sometimes called a penalty parameter, controls the strength of the penalty term in ridge regression and lasso regression. It is basically the amount of shrinkage, where data values are shrunk towards a central point, like the mean.\n",
    "## In lasso regression, the hyperparameter lambda (λ), also known as the L1 penalty, balances the tradeoff between bias and variance in the resulting coefficients. As λ increases, the bias increases, and the variance decreases, leading to a simpler model with fewer parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7609b7-65fa-436e-803f-850e9a77199b",
   "metadata": {},
   "source": [
    "## Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a973fbf5-2e4b-4661-a773-36bc08b5e970",
   "metadata": {},
   "source": [
    "##  Lasso Regression can handle non-linear regression problems to some extent, but its primary strength lies in feature selection rather than modeling non-linear relationships. If the relationship between the features and the target variable is highly non-linear, it may be more appropriate to use other regression techniques, such as polynomial regression or non-linear regression models like decision trees, support vector regression, or neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59d729a-4784-4d35-880f-d41cfeed93a5",
   "metadata": {},
   "source": [
    "## Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee72ad34-9fb3-43dd-9506-d77fc2d70bba",
   "metadata": {},
   "source": [
    "## The main difference between Ridge Regression and Lasso Regression lies in the type of regularization penalty used. Ridge Regression uses L2 regularization, which adds the sum of squared coefficients multiplied by a regularization parameter to the cost function. Lasso Regression, on the other hand, uses L1 regularization, which adds the sum of the absolute values of the coefficients multiplied by a regularization parameter to the cost function. The L1 regularization has the effect of setting some coefficients exactly to zero, resulting in feature selection, whereas L2 regularization only shrinks the coefficients towards zero but does not eliminate them entirely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7308244f-17ed-472d-8711-a44b7399fb78",
   "metadata": {},
   "source": [
    "## Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1195d3d7-da5f-4832-b231-66de86a881d2",
   "metadata": {},
   "source": [
    "## Yes, Lasso Regression can handle multicollinearity in the input features.\n",
    "## 1. Feature Selection\n",
    "\n",
    "## Lasso Regression: Performs feature selection by setting coefficients of less important features to zero.\n",
    "\n",
    "\n",
    "## 2. Handling Multicollinearity\n",
    "\n",
    "## Lasso Regression: Tends to select only one variable from a group of highly correlated variables, potentially leading to suboptimal performance.\n",
    "##  Multicollinearity refers to the presence of high correlation between independent variables. Lasso Regression's regularization penalty has the effect of shrinking the coefficients, and in the presence of multicollinearity, it tends to distribute the importance among the correlated features. As a result, some of the correlated features may have their coefficients reduced to zero, effectively selecting one feature over others. This way, Lasso Regression can indirectly handle multicollinearity by providing a mechanism for feature selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f3a82-0b4c-421d-9f55-2744f0773f2e",
   "metadata": {},
   "source": [
    "## Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a67fc5-cd61-43a5-bd35-7ed1a3186173",
   "metadata": {},
   "source": [
    "##  The optimal value of the regularization parameter (lambda) in Lasso Regression is typically chosen using techniques like cross-validation. Cross-validation involves splitting the data into multiple training and validation sets, fitting the model with different values of lambda on the training sets, and evaluating the model's performance on the validation sets. The value of lambda that gives the best performance, as determined by a suitable evaluation metric (e.g., mean squared error, R-squared), is considered the optimal value. Alternatively, techniques like grid search or model selection algorithms can be used to automate the process of selecting the optimal lambda value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7d7b7a-2899-4c0f-9d03-f1e8475dafe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
