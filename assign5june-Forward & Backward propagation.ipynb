{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05bde54f-d8d9-4fc2-b924-58850a3f2f55",
   "metadata": {},
   "source": [
    "Q1. `What is the purpose of forward propagation in a neural network?`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3b7f60-1248-4667-a7c5-88c7d061ddc6",
   "metadata": {},
   "source": [
    "Ans:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27654f6f-8dfb-4b9f-9a44-27fca25f0f0d",
   "metadata": {},
   "source": [
    "The purpose of forward propagation in a neural network is to compute the output of the network given a specific input. During forward propagation, the input data is fed through the layers of the neural network in a sequential manner, and computations are performed to obtain the final output.\n",
    "\n",
    " `step-by-step explanation of the forward propagation process:`\n",
    "\n",
    "1. `Input Data:` The forward propagation starts with the input data, which is typically represented as a vector or a multi-dimensional tensor.\n",
    "\n",
    "2. `Weighted Sum and Activation:` As the input data passes through each layer of the neural network, the neurons in the layer perform two main computations:\n",
    "\n",
    "   `A. Weighted Sum:`The input data is multiplied by a set of learnable parameters called weights, and a bias term is added. This computation results in a weighted sum of the inputs.\n",
    "   \n",
    "   `B. Activation Function:` The weighted sum is then passed through an activation function, which introduces non-linearity into the neural network. Common activation functions include ReLU (Rectified Linear Unit), sigmoid, and tanh.\n",
    "\n",
    "3. `Output Layer:` The output of the final layer, also known as the output layer, represents the predicted result or classification made by the neural network for a given input.\n",
    "\n",
    "4. `Loss Calculation:` In supervised learning tasks such as classification or regression, the predicted output is compared to the ground truth (target) to compute a loss value. The loss quantifies how well the model is performing on the current input.\n",
    "\n",
    "5. `Backpropagation:` Once the forward propagation is complete and the loss is calculated, the neural network uses the backpropagation algorithm to adjust its weights and biases to minimize the loss and improve its predictions.\n",
    "\n",
    "The forward propagation process is fundamental to the functioning of a neural network. It enables the model to process input data and generate meaningful predictions. The weights and biases learned during forward and backward propagation together constitute the parameters of the neural network, and these parameters are continuously updated through training to improve the model's performance on the given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c135e7-9835-4b7c-975c-5a10f83040d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d7aaef7-913e-4aff-8246-81901374ce3c",
   "metadata": {},
   "source": [
    "Q2. `How is forward propagation implemented mathematically in a single-layer feedforward neural network?`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d92231-d759-4185-a51e-2ba71996aa98",
   "metadata": {},
   "source": [
    "Ans:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255fca47-2a03-4fdc-bec5-e5aad1828ba9",
   "metadata": {},
   "source": [
    "In a single-layer feedforward neural network, also known as a perceptron, forward propagation is relatively straightforward as there is only one layer of neurons. The input is directly connected to the output layer, and the computations involve simple matrix operations. Here's how forward propagation is implemented mathematically in a single-layer feedforward neural network:\n",
    "\n",
    "1. `Input Data:` Let's assume we have 'n' input features, represented as a vector 'X' of size (n, 1). Each element of the vector corresponds to one input feature.\n",
    "\n",
    "2. `Weights and Biases:` The single-layer neural network has a weight matrix 'W' of size (1, n) and a bias vector 'b' of size (1, 1). The weight matrix 'W' contains the learnable parameters that connect each input feature to the output neuron.\n",
    "\n",
    "3. `Weighted Sum:` The weighted sum 'Z' is calculated by taking the dot product of the weight matrix 'W' with the input vector 'X' and adding the bias 'b':\n",
    "\n",
    "   Z = W * X + b\n",
    "\n",
    "   Here, 'Z' is a scalar value representing the weighted sum of the inputs.\n",
    "\n",
    "4. `Activation Function:` After calculating the weighted sum, the output 'A' is obtained by passing 'Z' through an activation function 'f', which introduces non-linearity to the model. Common activation functions include ReLU, sigmoid, and tanh:\n",
    "\n",
    "   A = f(Z)\n",
    "\n",
    "   The output 'A' is the prediction made by the single-layer neural network for the given input.\n",
    "\n",
    "In summary, the forward propagation in a single-layer feedforward neural network involves the following steps:\n",
    "\n",
    "1. Compute the weighted sum 'Z' by taking the dot product of the input vector 'X' and the weight matrix 'W', and adding the bias 'b'.\n",
    "\n",
    "2. Pass the weighted sum 'Z' through an activation function 'f' to obtain the output 'A'.\n",
    "\n",
    "3. The output 'A' represents the prediction made by the single-layer neural network for the given input.\n",
    "\n",
    "During the training process, the weights 'W' and bias 'b' are updated through backpropagation to minimize the error between the predicted output 'A' and the ground truth. This process continues iteratively until the model converges to a set of optimal weights and biases that produce accurate predictions for the given task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3b2fd4-0763-4ec9-ab8c-ab3317b9750c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59578b25-0acb-40f7-a2a6-92e40c8e1306",
   "metadata": {},
   "source": [
    "Q3. `How are activation functions used during forward propagation?`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6301f544-ebc9-4010-884c-a334ba1a111a",
   "metadata": {},
   "source": [
    "Ans:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f2117f-fdb8-4e92-8584-db4357dcc023",
   "metadata": {},
   "source": [
    "During forward propagation in a neural network, activation functions are applied to the weighted sum of the inputs at each neuron to introduce non-linearity to the model. The purpose of activation functions is to determine the output of a neuron based on its input and decide whether the neuron should be activated (fire) or not. This process allows neural networks to learn complex patterns and relationships in the data.\n",
    "\n",
    "Here's how activation functions are used during forward propagation:\n",
    "\n",
    "1. `Weighted Sum Calculation:` During forward propagation, the input features are multiplied by their corresponding weights, and the sum of these weighted inputs is computed. This sum is also known as the \"logit\" or \"pre-activation\" value.\n",
    "\n",
    "2. `Activation Function:` After computing the weighted sum, the output of the neuron is obtained by passing the logit through an activation function. The activation function applies a non-linear transformation to the logit, generating the neuron's final output.\n",
    "\n",
    "3. `Non-Linearity:` The key role of the activation function is to introduce non-linearity into the neural network. Without activation functions, the entire network would behave like a linear model, and its ability to approximate complex functions would be severely limited. Non-linear activation functions allow neural networks to capture complex patterns and relationships in the data.\n",
    "\n",
    "`Common Activation Functions:`\n",
    "There are several activation functions used in neural networks, each with its characteristics. Some of the most commonly used activation functions include:\n",
    "\n",
    "1. `ReLU (Rectified Linear Unit):` f(x) = max(0, x)\n",
    "   - It is widely used due to its simplicity and computational efficiency.\n",
    "   - It is non-linear and effective at mitigating the vanishing gradient problem.\n",
    "\n",
    "2. `Sigmoid:` f(x) = 1 / (1 + exp(-x))\n",
    "   - It squashes the output between 0 and 1, which makes it suitable for binary classification problems.\n",
    "   - However, it suffers from the vanishing gradient problem, leading to slow convergence during training.\n",
    "\n",
    "3. `Tanh (Hyperbolic Tangent):` f(x) = (2 / (1 + exp(-2x))) - 1\n",
    "   - Similar to the sigmoid, but its output ranges from -1 to 1, providing a centered output around zero.\n",
    "   - It also suffers from the vanishing gradient problem but less severe compared to the sigmoid.\n",
    "\n",
    "4. `Leaky ReLU:` f(x) = max(a*x, x)   (where 'a' is a small positive constant)\n",
    "   - It addresses the \"dying ReLU\" problem by allowing a small negative slope for negative inputs.\n",
    "   - This prevents neurons from being completely inactive during training.\n",
    "\n",
    "By applying activation functions during forward propagation, neural networks can capture complex patterns and relationships in the data, enabling them to learn and make predictions on various types of tasks, such as image recognition, natural language processing, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c626ad-a27e-4ff9-bbb7-3a8e2344010c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97b57551-2f3c-40bf-b510-fef7bb84c07c",
   "metadata": {},
   "source": [
    "Q4. `What is the role of weights and biases in forward propagation?`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b27d0-054e-4b58-a12b-04a9c6cf0bc6",
   "metadata": {},
   "source": [
    "Ans:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94a1d0e-835f-4853-a58d-eeebfe08dfc8",
   "metadata": {},
   "source": [
    "In forward propagation, the role of weights and biases is to determine the output of each neuron in a neural network. These parameters are essential for the neural network to learn and make predictions on various tasks. Let's understand the role of weights and biases in more detail:\n",
    "\n",
    "1. `Weights:`\n",
    "   - Weights are the learnable parameters in a neural network that connect the neurons between two consecutive layers.\n",
    "   - Each connection between two neurons is associated with a weight, which represents the strength of that connection.\n",
    "   - During training, the neural network adjusts these weights to minimize the prediction error and improve its performance on the given task.\n",
    "   - The weights essentially control the impact of each input feature on the output of the neuron. By adjusting the weights, the network can learn to       assign higher or lower importance to different features.\n",
    "\n",
    "2. `Biases:`\n",
    "   - Biases are also learnable parameters associated with each neuron in the network (except for the input layer, which does not have biases).\n",
    "   - A bias is like an intercept term in a linear equation. It helps the neuron to adjust its output regardless of the input values.\n",
    "   - Biases allow the neural network to capture patterns even when the input features are equal to zero.\n",
    "   - Similar to weights, biases are also adjusted during training to optimize the performance of the neural network.\n",
    "\n",
    "`Forward Propagation Process:`\n",
    "1. `Input Layer:` The input layer receives the raw data or features, and each neuron in the input layer corresponds to one feature.\n",
    "\n",
    "2. `Weighted Sum:` During forward propagation, the inputs are multiplied by their corresponding weights, and the sum of these weighted inputs is calculated for each neuron in the hidden layers.\n",
    "\n",
    "3. `Bias Addition:` After the weighted sum is calculated, the bias term (if present) is added to the result.\n",
    "\n",
    "4. `Activation Function:` The final output of each neuron is obtained by passing the result through an activation function. The activation function introduces non-linearity into the network, allowing it to learn complex patterns and relationships in the data.\n",
    "\n",
    "5. `Output Layer:` The output layer produces the final prediction of the neural network, which could be in the form of class probabilities for classification tasks or continuous values for regression tasks.\n",
    "\n",
    "By adjusting the weights and biases during the training process, the neural network learns to make accurate predictions on the given data and improves its performance on the task at hand. The optimization of these parameters is achieved through backpropagation, where the network's performance is evaluated using a loss function, and the gradients of the loss with respect to the weights and biases are computed to update these parameters accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f5094c-67e7-4eaa-96f1-434b90bf5787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c8ce28d-e4d5-4f24-905f-54bb4ef752bb",
   "metadata": {},
   "source": [
    "Q5. `What is the purpose of applying a softmax function in the output layer during forward propagation?`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a530db-9bfe-40ec-96f2-efd40c168bce",
   "metadata": {},
   "source": [
    "Ans:-"
   ]
  },
  {
   "attachments": {
    "b8bcaf83-2437-433c-8c19-01b669611b86.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "2421f004-1c33-4589-92c0-56a7f9f45c3a",
   "metadata": {},
   "source": [
    "The purpose of applying a softmax function in the output layer during forward propagation is to convert the raw scores or logits of the output neurons into a probability distribution over multiple classes. The softmax function is commonly used in multi-class classification problems to generate class probabilities that sum up to 1, allowing us to interpret the output as the likelihood of each class being the correct prediction.\n",
    "\n",
    "`Mathematically, the softmax function is defined as follows for the output of a neuron j in the output layer:`\n",
    "![image.png](attachment:b8bcaf83-2437-433c-8c19-01b669611b86.png)\n",
    "\n",
    "Where:\n",
    "- \\( z_j \\) is the raw score or logit of the neuron j.\n",
    "- \\( e \\) is the base of the natural logarithm (Euler's number).\n",
    "- \\( K \\) is the number of classes in the problem.\n",
    "\n",
    "Applying the softmax function to each output neuron's raw score ensures that the output values fall between 0 and 1, and the sum of the probabilities across all classes is equal to 1.\n",
    "\n",
    "`The softmax function has two main benefits:`\n",
    "\n",
    "1. `Probability Interpretation:` After applying the softmax function, the output values represent the probability that each class is the correct prediction given the input. This allows us to interpret the model's confidence in its predictions and choose the class with the highest probability as the final prediction.\n",
    "\n",
    "2. `Better Optimization:` The softmax function introduces non-linearity into the model, making the loss surface more suitable for optimization using techniques like gradient descent. It helps the model to learn better representations and improve the convergence of the optimization process.\n",
    "\n",
    "In summary, applying the softmax function in the output layer is crucial for obtaining meaningful and interpretable probabilities for multi-class classification tasks, enabling us to make confident predictions based on the output probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3b84ad-e6aa-4abc-ad33-4b8a6d227a50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4fa215f-ecf7-48d9-af46-f75fe504d4c3",
   "metadata": {},
   "source": [
    "Q6.`What is the purpose of backward propagation in a neural network?`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6813f7f-1432-4e87-8024-07fa10abd4c6",
   "metadata": {},
   "source": [
    "Ans:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c525a2-8e89-4c6f-8b2e-7b14173d21b6",
   "metadata": {},
   "source": [
    "The purpose of backward propagation (also known as backpropagation) in a neural network is to update the model's parameters (weights and biases) based on the computed gradients of the loss function with respect to those parameters. Backpropagation is an essential step in the training process of neural networks, as it allows the model to learn from the training data and improve its performance over time.\n",
    "\n",
    "During forward propagation, the input data is fed through the neural network, and the output predictions are computed. Once the predictions are obtained, the loss function is used to measure the error between the predicted values and the actual labels in the training data.\n",
    "\n",
    "`Backward propagation involves the following steps:`\n",
    "\n",
    "1. `Compute the Loss Gradient:` The first step is to calculate the gradient of the loss function with respect to the output predictions. This gradient represents the rate of change of the loss function concerning the model's predictions.\n",
    "\n",
    "2. `Propagate Gradients Backwards:` The gradient is then propagated backward through the layers of the neural network. For each layer, the gradients of the loss function with respect to the weights and biases of that layer are computed. This is done using the chain rule of calculus, which allows us to break down the gradients of the loss function with respect to the model's parameters into smaller gradients for each layer.\n",
    "\n",
    "3. `Update Model Parameters:` After calculating the gradients, the model's parameters are updated using an optimization algorithm (e.g., gradient descent or its variants). The goal is to find the optimal set of parameters that minimizes the loss function and improves the model's performance on the training data.\n",
    "\n",
    "By iteratively performing forward and backward propagation on batches of training data, the neural network learns from the training examples and adjusts its parameters to minimize the error between predictions and actual labels. This process continues until the model converges to a set of parameters that generalizes well to new, unseen data.\n",
    "\n",
    "In summary, backward propagation is a crucial step in the training process of neural networks. It enables the model to learn from its mistakes by updating the parameters based on the computed gradients, allowing the network to improve its performance and make better predictions over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27372998-0ab6-4192-b20f-5ee443a7e6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d8e60d4-da01-4c7e-ad6d-f2900a594840",
   "metadata": {},
   "source": [
    "Q7. `How is backward propagation mathematically calculated in a single-layer feedforward neural network?`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf71f087-a5bf-4d3d-9425-3320516afce1",
   "metadata": {},
   "source": [
    "Ans:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4142de-4901-4cb6-8c31-6896ee74a309",
   "metadata": {},
   "source": [
    "In a single-layer feedforward neural network, backward propagation is mathematically calculated using the principles of calculus, specifically the chain rule. The goal of backward propagation is to compute the gradients of the loss function with respect to the model's parameters (weights and biases) so that these parameters can be updated to minimize the loss.\n",
    "\n",
    "`Let's consider a single-layer feedforward neural network with the following components:`\n",
    "\n",
    "- Input features: denoted as x (a vector of input features)\n",
    "- Weights: denoted as W (a matrix of weights connecting the input features to the output)\n",
    "- Biases: denoted as b (a vector of biases added to the weighted sum of inputs)\n",
    "- Activation function: denoted as f (applied element-wise to the weighted sum of inputs)\n",
    "\n",
    "`The output of the network can be expressed as:`\n",
    "\n",
    "y = f(W * x + b)\n",
    "\n",
    "Now, let's assume we have a loss function L(y_true, y_pred) that measures the error between the true labels (y_true) and the predicted output (y_pred). The goal is to calculate the gradients of the loss function with respect to the weights and biases.\n",
    "\n",
    "1. `Calculate the Gradient of the Loss with Respect to the Predicted Output (dy_pred):`\n",
    "The gradient of the loss with respect to the predicted output (dy_pred) is calculated as:\n",
    "\n",
    "dy_pred = dL / dy_pred\n",
    "\n",
    "This gradient represents how the loss changes concerning the predicted output.\n",
    "\n",
    "2. `Calculate the Gradient of the Predicted Output with Respect to the Weighted Sum of Inputs (dz):`\n",
    "The gradient of the predicted output with respect to the weighted sum of inputs (dz) is calculated using the derivative of the activation function:\n",
    "\n",
    "dz = df(W * x + b) / d(W * x + b)\n",
    "\n",
    "3. `Calculate the Gradient of the Loss with Respect to the Weights (dW) and Biases (db):`\n",
    "Using the chain rule, we can calculate the gradients of the loss with respect to the weights and biases as follows:\n",
    "\n",
    "dW = dy_pred * dz * x.T\n",
    "db = dy_pred * dz\n",
    "\n",
    "where x.T is the transpose of the input features.\n",
    "\n",
    "4.` Update the Weights and Biases:`\n",
    "Finally, we use the gradients calculated in step 3 to update the weights and biases using an optimization algorithm such as gradient descent or its variants.\n",
    "\n",
    "By repeatedly performing forward propagation to compute the output and backward propagation to calculate the gradients and update the parameters, the single-layer feedforward neural network learns from the data and adjusts its weights and biases to minimize the loss function and make accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11f5328-6266-4489-86ab-8e7c861124d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d3698d4-c1f7-4545-ab44-98e95ce3d771",
   "metadata": {},
   "source": [
    "Q8. `Can you explain the concept of the chain rule and its application in backward propagation?`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c38926-712c-4bec-96d8-6b48d9362a80",
   "metadata": {},
   "source": [
    "Ans:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951dc599-b382-40e8-aa93-6e3ce1c096ed",
   "metadata": {},
   "source": [
    "The chain rule is a fundamental concept in calculus that allows us to calculate the derivative of a composite function. In the context of neural networks and backward propagation, the chain rule plays a crucial role in computing gradients of complex functions, which is essential for updating the model's parameters during the training process.\n",
    "\n",
    "Let's consider a neural network with multiple layers and a loss function that measures the error between the true labels and the predicted output. The goal of backward propagation is to calculate the gradients of the loss function with respect to the model's parameters (weights and biases) so that these parameters can be updated to minimize the loss.\n",
    "\n",
    "During forward propagation, the network computes the predicted output by passing the input data through a series of layers, each comprising a weighted sum of inputs followed by an activation function. The output of one layer serves as the input to the next layer. The chain rule enables us to calculate how the gradients of the loss function at the output layer propagate backward through each layer to determine how each parameter affects the overall loss.\n",
    "\n",
    "Mathematically, the chain rule states that if we have a composite function g(f(x)), where f(x) is the output of one function and g(f) is the output of another function, then the derivative of the composite function with respect to x is given by:\n",
    "\n",
    "`(d(g(f(x))) / dx) = (dg / df) * (df / dx)`\n",
    "\n",
    "In the context of neural networks, this means that to calculate the gradient of the loss function with respect to a parameter in a specific layer, we need to multiply the local gradient of the loss with respect to the output of that layer (dg / df) with the gradient of the output of that layer with respect to the parameter (df / dx).\n",
    "\n",
    "The chain rule is applied iteratively during backward propagation, starting from the output layer and moving backward through each layer of the neural network. At each layer, the local gradient is calculated based on the derivative of the activation function, and the gradient of the loss with respect to the parameters is updated using the local gradient and the gradient from the previous layer.\n",
    "\n",
    "By applying the chain rule in backward propagation, we efficiently calculate the gradients of the loss function with respect to all the parameters in the network. These gradients are then used to update the parameters during optimization, allowing the neural network to learn from the data and improve its performance over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99e2c22-7d61-4e3e-9fcb-ab860f945e32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac069ec7-7f62-4ee6-9c19-50dd68767e0e",
   "metadata": {},
   "source": [
    "Q9. `What are some common challenges or issues that can occur during backward propagation, and how can they be addressed?`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cae7dcc-7eb1-426f-a5ea-8ae1251df7a8",
   "metadata": {},
   "source": [
    "Ans:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfe2f19-91f4-4015-8154-ad55804e1257",
   "metadata": {},
   "source": [
    "During backward propagation in neural networks, several challenges or issues can occur, which can affect the training process and the performance of the model. Here are some common challenges and ways to address them:\n",
    "\n",
    "1. `Vanishing or Exploding Gradients:` The gradients of the loss function with respect to the parameters may become very small (vanishing gradients) or very large (exploding gradients) as they are propagated back through the layers. This can lead to slow convergence or instability in training.\n",
    "\n",
    "   Solution: Use proper weight initialization techniques like Xavier/Glorot initialization, which can help mitigate the vanishing/exploding gradient problem. Additionally, consider using activation functions that do not saturate (e.g., ReLU) as they can alleviate vanishing gradients.\n",
    "\n",
    "2. `Numerical Instability:` In deep neural networks, numerical precision issues can occur when very large or very small values are involved in computations. This can lead to NaN or Infinity values.\n",
    "\n",
    "   Solution: Implement gradient clipping, where gradients that exceed a certain threshold are clipped, preventing them from becoming too large. This helps stabilize training and prevents numerical instability.\n",
    "\n",
    "3. `Local Minima and Plateaus:` The loss landscape in high-dimensional parameter space may contain many local minima or plateaus, making it challenging to find the global minimum.\n",
    "\n",
    "   Solution: Use optimization techniques that are less sensitive to local minima, such as adaptive optimization algorithms (e.g., Adam) that adjust the learning rate for each parameter individually. Additionally, exploring different learning rates and batch sizes can help escape local minima.\n",
    "\n",
    "4. `Overfitting:` Backward propagation can lead to overfitting, where the model performs well on the training data but poorly on unseen data.\n",
    "\n",
    "   Solution: Apply regularization techniques like L1 or L2 regularization to penalize large weights and prevent overfitting. Dropout is another regularization technique that randomly drops neurons during training to prevent co-adaptation of neurons.\n",
    "\n",
    "5. `Computational Complexity:` Backward propagation involves computing gradients for all the parameters in the model, which can be computationally expensive, especially for deep networks.\n",
    "\n",
    "   Solution: Implement mini-batch stochastic gradient descent (SGD), which updates the parameters using a small subset of the training data at each step. This reduces the computational burden and speeds up training.\n",
    "\n",
    "6. `Batch Size Selection:` The choice of batch size can impact the convergence and generalization of the model.\n",
    "\n",
    "   Solution: Experiment with different batch sizes and find a balance between computational efficiency and model performance. Smaller batch sizes can lead to noisy gradients but faster convergence, while larger batch sizes provide more stable gradients but may be slower.\n",
    "\n",
    "By being aware of these challenges and implementing appropriate techniques to address them, we can improve the stability and efficiency of backward propagation, leading to better training performance and more accurate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec41d7ec-a2fd-43c0-ab04-dc30a7d79c72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
