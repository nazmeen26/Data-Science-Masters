{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99161e2c-b8c8-45ac-bf30-72398c073163",
   "metadata": {},
   "source": [
    "## `Topic: understanding Pooling and padding in CNN: `\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f47a9eb-2fb1-4fbd-beb0-8344b2883e1f",
   "metadata": {},
   "source": [
    "### 1. `Describe the purpose and benefits of pooling in CNN.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9355382-dbc7-4468-9f09-c82a0bc5f878",
   "metadata": {},
   "source": [
    "`Purpose and Benefits of Pooling in CNN:`\n",
    "Pooling is a down-sampling operation commonly used in Convolutional Neural Networks (CNNs) to reduce the spatial dimensions of the feature maps while preserving important information. The main purposes and benefits of pooling are:\n",
    "\n",
    "-- `Spatial Down-Sampling:` Pooling reduces the size of the feature maps, making the model more computationally efficient and reducing the memory requirements. This is especially important when dealing with large images or deep architectures.\n",
    "\n",
    "-- `Translation Invariance:` Pooling makes the model more robust to slight translations of the object within the image. By selecting the most dominant features within a pooling window, the specific position of the feature becomes less critical, enhancing the model's ability to recognize objects irrespective of their location in the input.\n",
    "\n",
    "--`Feature Reduction:` Pooling helps to reduce the number of parameters and avoid overfitting. By discarding less significant details and retaining only the most important features, pooling encourages the model to focus on the most relevant information in the input.\n",
    "\n",
    "--`Increased Receptive Field:` Pooling helps to increase the receptive field of the neurons in deeper layers of the network. By reducing the spatial dimensions, neurons in deeper layers can capture information from a larger portion of the input image, facilitating the learning of more complex patterns and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac3643-bddd-4183-a61d-544b5d09d4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b68ada45-1408-49de-a226-87c2727db18d",
   "metadata": {},
   "source": [
    "### 2. `Explain the difference between min pooling and max pooling.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987fcef3-9281-4184-9a2e-936eda55d720",
   "metadata": {},
   "source": [
    "Min pooling and max pooling are two types of pooling operations commonly used in Convolutional Neural Networks (CNNs) for down-sampling feature maps. Both pooling methods aim to reduce the spatial dimensions of the feature maps while preserving important information. However, they differ in how they select values within the pooling window.\n",
    "\n",
    "--`Max Pooling:`\n",
    "Max pooling is the most common type of pooling operation in CNNs.\n",
    "In max pooling, the maximum value within the pooling window is selected and retained, while all other values are discarded.\n",
    "Max pooling is used to focus on the most dominant features within the feature maps. By selecting the maximum value, it ensures that the most significant feature in the pooling window is preserved in the down-sampled feature map.\n",
    "Max pooling helps to enhance the model's ability to recognize important patterns and features in the input, making it more robust to slight variations in position or translation of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27695393-ed35-4345-a418-17c602341827",
   "metadata": {},
   "outputs": [],
   "source": [
    "Example:\n",
    "Consider a 2x2 max pooling window applied to the following 4x4 input feature map:\n",
    "\n",
    "Input Feature Map:\n",
    "[ 1,  3,  2,  4]\n",
    "[ 6,  8,  9,  5]\n",
    "[12, 11, 10,  7]\n",
    "[16, 15, 14, 13]\n",
    "\n",
    "Max Pooling Output:\n",
    "[ 8,  9]\n",
    "[16, 15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3567a14-cc6d-405a-9dc3-9145ad777159",
   "metadata": {},
   "source": [
    "--`Min Pooling:`\n",
    "Min pooling is less common than max pooling and is not as widely used in CNNs.\n",
    "In min pooling, the minimum value within the pooling window is selected and retained, while all other values are discarded.\n",
    "Min pooling aims to focus on the least dominant features within the feature maps. By selecting the minimum value, it emphasizes the less significant features in the pooling window.\n",
    "Min pooling may not be as effective as max pooling in capturing important patterns and features, and it may not be as suitable for most computer vision tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8342914-12f5-4419-a5b3-75561ed1e7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Example:\n",
    "Consider a 2x2 min pooling window applied to the same 4x4 input feature map as above:\n",
    "    \n",
    "Min Pooling Output:\n",
    "[ 1,  2]\n",
    "[12, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f753018d-85b7-4a33-a54c-eb4fdb636f01",
   "metadata": {},
   "source": [
    "Min pooling and max pooling are two common types of pooling operations in CNNs:\n",
    "\n",
    "`Max Pooling:` In max pooling, the maximum value within the pooling window is selected and retained, while the rest of the values are discarded. Max pooling is widely used as it helps the model focus on the most dominant features in the feature maps.\n",
    "\n",
    "`Min Pooling:` In min pooling, the minimum value within the pooling window is selected and retained. Min pooling is less common and not as popular as max pooling, as it may not capture the most relevant features in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eada33c1-399a-4515-b299-71f2774ec8ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ab05a47-462c-4577-9b61-3fbed54f59fd",
   "metadata": {},
   "source": [
    "### 3. `Discuss the concept of padding in CNN and its significance.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca644ecd-f45f-42da-b995-ec9315f8f40b",
   "metadata": {},
   "source": [
    "Concept of Padding in CNN and Its Significance:\n",
    "\n",
    "Padding is the process of adding extra pixels around the borders of the input image or feature maps before applying convolution or pooling operations. The main significance of padding is to control the size of the output feature maps and preserve information at the edges of the input:\n",
    "\n",
    "`Avoiding Information Loss:` Without padding, convolutional and pooling operations reduce the spatial dimensions of the feature maps, leading to information loss at the edges. Padding helps retain the spatial dimensions and prevents the output feature maps from being smaller than the input.\n",
    "\n",
    "`Maintaining Spatial Information:` Padding ensures that the convolutional filters or pooling windows can be applied to all locations of the input image, even at the borders. This maintains the spatial information and allows the model to learn relevant features across the entire image.\n",
    "\n",
    "`There are two common types of padding:`\n",
    "\n",
    "a. `Zero-padding:` In zero-padding, extra pixels with a value of zero are added around the input image or feature map. Zero-padding is the most widely used type of padding and is preferred due to its simplicity and effectiveness.\n",
    "\n",
    "b. `Reflective padding:` In reflective padding (also called symmetric padding), the values at the border of the input are mirrored and extended beyond the boundary. This type of padding can be useful for certain applications but is less commonly used compared to zero-padding.\n",
    "\n",
    "In summary, padding is a crucial technique in CNNs that helps preserve spatial information, control the size of feature maps, and ensure that the convolutional and pooling operations produce accurate and meaningful representations of the input data. It plays a significant role in improving the performance and robustness of CNN models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373b6057-2bba-44a7-ac30-755337c523ca",
   "metadata": {},
   "source": [
    "### 4. `compare and contrast zero-padding and valid-padding in terms of their objects on the output feature map size.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f32315-6b7d-4161-a91c-aab99f36a79c",
   "metadata": {},
   "source": [
    "Zero-padding and valid-padding are two common types of padding used in Convolutional Neural Networks (CNNs),\n",
    "\n",
    "`Zero-padding:`\n",
    "\n",
    "-- Zero-padding involves adding extra pixels with a value of zero around the input image or feature map.\n",
    "\n",
    "-- With zero-padding, the spatial dimensions of the output feature map remain the same as the input or a specified size, depending on the amount of padding added.\n",
    "\n",
    "-- Zero-padding is especially useful when the goal is to maintain the spatial information and spatial size of the feature maps during convolution and pooling operations.\n",
    "\n",
    "-- For a given convolutional kernel size, the output feature map size is larger than the input size because of the added zero-pixels around the border.\n",
    "\n",
    "-- Zero-padding can help reduce the border effects and capture features accurately, especially at the edges of the input.\n",
    "\n",
    "--Zero-padding is commonly used in CNN architectures to ensure consistency in feature map sizes and enable better learning.\n",
    "\n",
    "`Valid-padding:`\n",
    "\n",
    "--Valid-padding involves no padding, which means no extra pixels are added around the input image or feature map.\n",
    "\n",
    "--With valid-padding, the spatial dimensions of the output feature map are reduced compared to the input, as the convolution kernel cannot be fully applied to the edges of the input.\n",
    "\n",
    "--Valid-padding is used when the goal is to reduce the spatial dimensions of the feature maps to extract more high-level and abstract features from the input.\n",
    "\n",
    "--For a given convolutional kernel size, the output feature map size is smaller than the input size because the convolution is restricted to the central pixels of the input.\n",
    "\n",
    "--Valid-padding is useful when the spatial information at the borders is less important, or when the objective is to downsample the feature maps for subsequent layers or tasks.\n",
    "\n",
    "--Valid-padding can help reduce the computational cost and memory requirements of the network since fewer operations are performed on the smaller feature maps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1300642d-d574-4b48-963f-5613ec6370ab",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77efa77e-b213-42d3-8ded-5f898c518c14",
   "metadata": {},
   "source": [
    "## `TOPIC: Exploring LeNet.`\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d8d8f3-0fe4-4133-a245-5985dc183e3d",
   "metadata": {},
   "source": [
    "### 1. `Provide a brief overview of LeNet-5 architecture.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bcdafa-b4a4-4df7-ae56-8a97ad0b4445",
   "metadata": {},
   "source": [
    "LeNet-5 is a convolutional neural network (CNN) architecture designed by Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner in 1998. It is one of the pioneering CNNs and played a crucial role in advancing the field of computer vision and deep learning. LeNet-5 was specifically designed for handwritten digit recognition, and it demonstrated impressive performance on the MNIST dataset.\n",
    "\n",
    "The architecture of LeNet-5 consists of seven layers, including three convolutional layers, two subsampling (pooling) layers, and two fully connected layers. \n",
    "\n",
    "`Here's a brief overview of each layer:`\n",
    "\n",
    "`Input layer:` The input to LeNet-5 is a grayscale image of size 32x32 pixels. The network expects a fixed-size input, so the images in the MNIST dataset were centered in a 32x32 pixel canvas.\n",
    "\n",
    "`First Convolutional layer:` The first convolutional layer has six feature maps (also known as channels). Each feature map is obtained by applying a 5x5 convolutional filter to the input image. The output of this layer is a set of six 28x28 feature maps.\n",
    "\n",
    "`First Subsampling (Pooling) layer:` The first subsampling layer performs average pooling over non-overlapping 2x2 regions for each feature map. This reduces the spatial dimensions of the feature maps to half, resulting in six 14x14 feature maps.\n",
    "\n",
    "`Second Convolutional layer:` The second convolutional layer has 16 feature maps, each obtained by applying a 5x5 convolutional filter to the output of the first subsampling layer. The result is a set of 16 10x10 feature maps.\n",
    "\n",
    "`Second Subsampling (Pooling) layer:` Similar to the first pooling layer, the second pooling layer performs average pooling over non-overlapping 2x2 regions for each feature map, reducing their spatial dimensions to half. The output is 16 5x5 feature maps.\n",
    "\n",
    "`Fully Connected layer:` The fully connected layers act as a traditional neural network. The 16 5x5 feature maps are flattened into a vector and fed into a fully connected layer with 120 neurons.\n",
    "\n",
    "`Output layer:` The final fully connected layer has 84 neurons, and it is connected to the output layer with 10 neurons, representing the digits 0 to 9. The softmax activation function is used in the output layer to produce a probability distribution over the 10 classes.\n",
    "\n",
    "LeNet-5 is characterized by its simplicity and effectiveness in image classification tasks, especially for handwritten digit recognition. It laid the foundation for the development of more complex CNN architectures that are widely used today for a wide range of computer vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49de1cd-bf62-437e-a81c-91d65745b145",
   "metadata": {},
   "source": [
    "### 2. `Describe the key components of LeNet-5 and their respective purposes.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb9eca6-1cb9-4e55-a28a-ad012835b4cc",
   "metadata": {},
   "source": [
    "The key components of LeNet-5 and their respective purposes are as follows:\n",
    "\n",
    "`Convolutional Layers:` The first and second convolutional layers are the fundamental building blocks of LeNet-5. They are responsible for learning local patterns and features from the input images. Each convolutional layer applies a set of learnable filters (kernels) to the input feature maps, convolving them to create new feature maps. The purpose of these layers is to extract low-level features like edges, corners, and textures from the input images.\n",
    "\n",
    "`Subsampling (Pooling) Layers:` LeNet-5 includes two subsampling (pooling) layers after each convolutional layer. The pooling layers perform spatial down-sampling and help reduce the spatial dimensions of the feature maps while retaining their important features. The most commonly used pooling operation in LeNet-5 is average pooling, which calculates the average value within a small region (e.g., 2x2) of the feature map. Pooling aids in reducing the number of parameters in the network, making it computationally efficient.\n",
    "\n",
    "`Fully Connected Layers:` After the convolutional and pooling layers, LeNet-5 includes two fully connected layers. These layers act as a traditional neural network, where all neurons are connected to every neuron in the previous layer. The fully connected layers are responsible for combining the high-level features learned from the previous layers to make the final classification decision. In LeNet-5, the fully connected layers have 120 and 84 neurons, respectively, before connecting to the output layer.\n",
    "\n",
    "`Output Layer:` The output layer of LeNet-5 is a fully connected layer with 10 neurons, representing the 10 possible classes in the MNIST dataset (digits 0 to 9). The softmax activation function is applied to the output layer, which converts the raw scores into a probability distribution over the classes. The class with the highest probability is considered the final prediction.\n",
    "\n",
    "`Activation Functions:` Throughout LeNet-5, a common activation function used is the hyperbolic tangent (tanh) function. The tanh activation introduces non-linearity to the model, allowing it to learn complex relationships between features and making it more capable of capturing intricate patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e905e9f-0ade-424f-9307-a52d72e82b92",
   "metadata": {},
   "source": [
    "### 3. `Discuss the advantages and limitations of LeNet-5 in the context of image classification tasks.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b89972b-6649-47b9-8151-867f2ee584dc",
   "metadata": {},
   "source": [
    " `Advantages and limitations of LeNet-5:`\n",
    " \n",
    "   `Advantages:`\n",
    "   - LeNet-5 introduced the concept of CNNs, demonstrating their effectiveness in image classification tasks.\n",
    "   - It is relatively simple compared to modern CNN architectures, making it easy to understand and implement.\n",
    "   - LeNet-5 achieved state-of-the-art performance on the MNIST dataset and paved the way for more advanced CNNs.\n",
    "\n",
    "   `Limitations:`\n",
    "   - LeNet-5 may struggle with more complex and high-resolution datasets due to its limited depth and simplicity.\n",
    "   - The sigmoid activation function used in LeNet-5 can suffer from the vanishing gradient problem, slowing down convergence.\n",
    "   - Its performance may not be competitive with modern CNN architectures like ResNet, VGG, or Inception on large-scale image datasets.\n",
    "\n",
    "4. Implementation and evaluation of LeNet-5:\n",
    "   To implement LeNet-5, you can use popular deep learning frameworks like TensorFlow or PyTorch. Train it on a publicly available dataset such as MNIST, a dataset of handwritten digits. Evaluate its performance using metrics like accuracy, precision, recall, and F1-score. The results will likely show that LeNet-5 performs well on MNIST but may struggle with more complex datasets. Additionally, you can compare its performance with other modern CNN architectures to highlight its limitations on larger and more diverse datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a733e87-d9d7-491c-83ee-8781a441249b",
   "metadata": {},
   "source": [
    "### 4. `Implement LeNet-5 using a deep learning framework of your choice (e.g., TensorFlow, PyTocch) and train it on a publicly available dataset (e.g., MNIST). Evaluate its performance and provide insights.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29a1f94c-793a-4399-b42f-809743d607d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.2355 - accuracy: 0.9287 - val_loss: 0.1027 - val_accuracy: 0.9662\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0868 - accuracy: 0.9734 - val_loss: 0.0714 - val_accuracy: 0.9771\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0598 - accuracy: 0.9819 - val_loss: 0.0651 - val_accuracy: 0.9797\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0471 - accuracy: 0.9854 - val_loss: 0.0467 - val_accuracy: 0.9839\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0376 - accuracy: 0.9877 - val_loss: 0.0399 - val_accuracy: 0.9861\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0308 - accuracy: 0.9900 - val_loss: 0.0416 - val_accuracy: 0.9855\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0243 - accuracy: 0.9921 - val_loss: 0.0411 - val_accuracy: 0.9864\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0215 - accuracy: 0.9932 - val_loss: 0.0435 - val_accuracy: 0.9883\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0197 - accuracy: 0.9937 - val_loss: 0.0439 - val_accuracy: 0.9873\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0161 - accuracy: 0.9946 - val_loss: 0.0447 - val_accuracy: 0.9881\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0447 - accuracy: 0.9881\n",
      "Test accuracy: 0.988099992275238\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to range [0, 1]\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Reshape the images to (num_samples, 28, 28, 1) as LeNet-5 takes input of size (32, 32, 1)\n",
    "train_images = tf.expand_dims(train_images, axis=-1)\n",
    "test_images = tf.expand_dims(test_images, axis=-1)\n",
    "\n",
    "# Build LeNet-5 architecture\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(6, kernel_size=(5, 5), activation='tanh', input_shape=(28, 28, 1)),\n",
    "    layers.AveragePooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(16, kernel_size=(5, 5), activation='tanh'),\n",
    "    layers.AveragePooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(120, activation='tanh'),\n",
    "    layers.Dense(84, activation='tanh'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c7c381-2687-43a6-b789-e75810551944",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327e4864-2e4a-44cf-a121-de42d874ffa0",
   "metadata": {},
   "source": [
    "## `Topic: Analyzing AlexNet`\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a8aed7-ae11-499f-9ed9-4b30f9df250b",
   "metadata": {},
   "source": [
    "### 1. `Present an overview of the AlexNet architecture.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9b6d15-a474-4341-a8f4-797b955fae84",
   "metadata": {},
   "source": [
    "AlexNet is a deep convolutional neural network (CNN) that gained significant attention and achieved a breakthrough in image classification during the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, the architecture demonstrated the power of deep learning and established the effectiveness of CNNs for image recognition tasks.\n",
    "\n",
    "`Overview of the AlexNet architecture:`\n",
    "\n",
    "`Input Layer:` The network takes an input image of size 224x224x3 (RGB color channels) as a tensor.\n",
    "\n",
    "Convolutional Layers (Conv1-Conv5): AlexNet consists of five convolutional layers, each followed by a ReLU activation function. These layers learn hierarchical features from the input image. The number of filters increases as we move deeper into the network. The first layer detects basic features like edges and gradients, while deeper layers capture more complex patterns.\n",
    "\n",
    "`Max Pooling Layers (Pool1-Pool3)`: After each convolutional layer, there is a max-pooling layer that reduces the spatial dimensions of the feature maps. Max-pooling selects the maximum value from a local region, helping to achieve translation invariance and reduce computational complexity.\n",
    "\n",
    "`Local Response Normalization (LRN) Layer:` AlexNet incorporates Local Response Normalization after the first and second convolutional layers. LRN enhances the network's generalization by introducing local competition among neurons and normalizing their responses.\n",
    "\n",
    "`Fully Connected Layers (FC1-FC3):` Following the convolutional and pooling layers, the feature maps are flattened and fed into three fully connected layers. These fully connected layers act as a traditional multi-layer perceptron, learning high-level representations from the extracted features.\n",
    "\n",
    "`Dropout Layers:` To mitigate overfitting, AlexNet includes dropout layers after the fully connected layers. Dropout randomly sets a fraction of the neurons' activations to zero during training, reducing co-adaptation and promoting better generalization.\n",
    "\n",
    "`Output Layer:` The final layer of the network is the output layer, usually comprising a softmax activation function for multi-class classification. It provides the probabilities for each class label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cf8319-7379-4d30-9d7f-ff87e5ca813a",
   "metadata": {},
   "source": [
    "### 2. `Explain the architectural innovations introduced in AlexNet that contributed to its breakthrough performance.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529f264d-73bd-48f0-82e3-e2b6662f6949",
   "metadata": {},
   "source": [
    "The success of AlexNet can be attributed to several key architectural innovations:\n",
    "\n",
    "a. `Deep architecture:` AlexNet was one of the first CNNs to have a deep architecture with multiple stacked convolutional layers. Prior to AlexNet, shallow networks were commonly used, but deeper architectures allowed the model to learn hierarchical features from the data.\n",
    "\n",
    "b. `ReLU activation function:` Instead of traditional activation functions like sigmoid or tanh, AlexNet used Rectified Linear Units (ReLU) as the activation function. ReLU helps to mitigate the vanishing gradient problem and accelerates convergence during training.\n",
    "\n",
    "c. `Overlapping pooling:` AlexNet introduced the concept of using overlapping max-pooling instead of traditional non-overlapping pooling. This contributed to better utilization of spatial information and provided a more robust representation.\n",
    "\n",
    "d. `Local Response Normalization (LRN):` LRN was used in the early layers of AlexNet to introduce local competition between neighboring neurons. It aids in generalization and improves the model's ability to handle variations in input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100633c8-9c70-42ec-9be7-c9fe0fc32f31",
   "metadata": {},
   "source": [
    "### `3.  Discuss the role of convolutional layers, pooling layers, and fully connected layers in AlexNet.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5d8314-73f0-4fed-bfda-2550826aa044",
   "metadata": {},
   "source": [
    "A. `Convolutional layers:` Convolutional layers are the building blocks of AlexNet. They perform the convolution operation by sliding small filters (kernels) over the input image to extract local features. These layers learn feature maps that represent patterns such as edges, textures, and simple shapes.\n",
    "\n",
    "B. `Pooling layers:` Pooling layers are used to reduce the spatial dimensions of the feature maps obtained from the convolutional layers. AlexNet uses max-pooling, which selects the maximum value from each local region of the feature map. This reduces the computational load and provides some degree of translation invariance.\n",
    "\n",
    "C. `Fully connected layers:` After the convolutional and pooling layers, the feature maps are flattened and fed into fully connected layers. These layers connect every neuron from one layer to every neuron in the next layer, allowing the network to learn high-level representations and make final predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae43717-7466-4cab-a2af-929efbf8ed2d",
   "metadata": {},
   "source": [
    "### `4. Implement AlexNet using a deep learning framework of your choice and evaluate its performance on a dataset of your choice.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3344219-eede-4913-9f69-b8a0d509e5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:19<00:00, 8842881.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "[1,   100] loss: 2.303\n",
      "[1,   200] loss: 2.302\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "#Load and preprocess the CIFAR-10 dataset:\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize(256),\n",
    "     transforms.RandomCrop(224),\n",
    "     transforms.RandomHorizontalFlip(),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "#Implement AlexNet architecture:\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = AlexNet(num_classes=10).to(device)\n",
    "#Define loss function and optimizer:\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "#Train the model:\n",
    "for epoch in range(10):  # Adjust the number of epochs as needed\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # Print every 100 mini-batches\n",
    "            print(f\"[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "#Evaluate the model on the test set:\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(\"Accuracy on the test set: %d %%\" % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd395409-c972-4f39-8812-f8ec2abf8345",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
