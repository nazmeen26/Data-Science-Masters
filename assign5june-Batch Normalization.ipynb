{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70819cb0-3614-4f0b-8c0f-da3d54ac6f59",
   "metadata": {},
   "source": [
    "### Objective: The objective of this assignment is to assess students' understanding of batch normalization in artificial neural networks (ANN) and its impact on training performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30badcec-7f22-4189-a99e-85187fb6c6a9",
   "metadata": {},
   "source": [
    "Q1. `Theory and concepts:`\n",
    "1. Explain the concept of batch normalization in the context of Artificial Neural Network\n",
    "2. Describe the benefits of using batch normalization during training\n",
    "3. Discuss the working principle of batch normalization, including the normalization step and the learnable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da51817-ad44-4b29-b1ff-ba3bedbd4e7b",
   "metadata": {},
   "source": [
    "`1. Concept of Batch Normalization in Artificial Neural Networks:`\n",
    "\n",
    "Batch Normalization is a technique used to normalize the activations of a layer's inputs in Artificial Neural Networks. It was introduced to address the internal covariate shift problem, which refers to the changing distribution of layer inputs during the training process. The idea behind batch normalization is to normalize the input data by adjusting and scaling the activations to have a mean of zero and a standard deviation of one. This normalization is performed over mini-batches of data during the training process.\n",
    "\n",
    "`2. Benefits of Using Batch Normalization During Training:`\n",
    "\n",
    "Batch Normalization offers several benefits that contribute to more stable and efficient training of neural networks:\n",
    "\n",
    "   A. `Faster Convergence:` By reducing internal covariate shift, batch normalization helps in faster convergence during training. This is especially crucial in deeper networks where training without batch normalization might require significantly more epochs to converge.\n",
    "\n",
    "   B. `Stabilized Gradients:` Batch normalization stabilizes the gradients during backpropagation, making the optimization process more reliable. It reduces the vanishing and exploding gradient problem, which can occur in deeper networks.\n",
    "\n",
    "   C. `Regularization Effect:` Batch normalization acts as a form of regularization. It adds some noise to the mini-batch statistics, which can help in reducing overfitting to some extent.\n",
    "\n",
    "   D. `Less Sensitive to Learning Rate:` Neural networks with batch normalization are less sensitive to the choice of learning rate. This makes it easier to tune hyperparameters during training.\n",
    "\n",
    "`3. Working Principle of Batch Normalization:`\n",
    "\n",
    "Batch normalization involves two main steps: normalization and scaling.\n",
    "\n",
    "   A. `Normalization Step:`\n",
    "      - Given a mini-batch of activations from a layer, compute the mean (μ) and standard deviation (σ) for each           feature over the mini-batch.\n",
    "      - Normalize the activations of each feature by subtracting the mean and dividing by the standard deviation.\n",
    "      - The normalized activations are given by the formula: z = (x - μ) / σ, where x is the input, μ is the mean,          and σ is the standard deviation.\n",
    "\n",
    "   B. `Scaling Step (Learnable Parameters):`\n",
    "      - After normalization, the activations are scaled and shifted using learnable parameters γ (scale) and β             (shift).\n",
    "      - The scaled and shifted activations are given by the formula: y = γ * z + β, where γ and β are learnable              parameters.\n",
    "\n",
    "The use of learnable parameters γ and β allows the network to adapt the normalized activations to the desired range and scale. During training, batch normalization updates the mini-batch statistics (mean and standard deviation) for each layer during each forward pass. However, during inference or testing, the final learned population statistics (calculated over the entire training dataset) are used to ensure consistency and reproducibility.\n",
    "\n",
    "In summary, batch normalization is a powerful technique that normalizes and scales the activations, leading to faster and more stable training of neural networks while providing some regularization effects to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86108f8-b386-4049-adbc-e869745de628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "474ba9c4-3680-4217-bfef-4beea8ba7295",
   "metadata": {},
   "source": [
    "Q2. `Implementation:`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1da1898-6313-4ac4-aade-9711ddf246d5",
   "metadata": {},
   "source": [
    "For this implementation, let's use the popular MNIST dataset of handwritten digits. We'll create a simple feedforward neural network using the PyTorch framework and compare its performance with and without batch normalization.\n",
    "\n",
    "`Step 1: Preprocessing the MNIST dataset**`\n",
    "\n",
    "First, we need to load and preprocess the MNIST dataset. We'll normalize the pixel values and split the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eb8c2c-f170-41b6-a7c1-92aff9400aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Data preprocessing\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load MNIST dataset\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ce0fa2-e3b4-403c-b584-8eed60d6ea7c",
   "metadata": {},
   "source": [
    "`Step 2: Implementing the Simple Feedforward Neural Network`\n",
    "\n",
    "We'll create a simple neural network with two hidden layers and an output layer. We'll use the ReLU activation function and cross-entropy loss for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f1e28b-e532-48d6-a224-1a84503b8e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)  # Flatten the input\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model_without_bn = SimpleNN()\n",
    "model_with_bn = SimpleNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c999b0-2b76-4ae1-8e82-f68cd2c40c8c",
   "metadata": {},
   "source": [
    "`Step 3: Training the Neural Network without Batch Normalization`\n",
    "\n",
    "We'll train the neural network without batch normalization and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21968592-e05c-4885-b6d2-66fdfeab1bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_without_bn = optim.SGD(model_without_bn.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Training loop without batch normalization\n",
    "for epoch in range(5):  # Number of epochs\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer_without_bn.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = model_without_bn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_without_bn.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print loss after each epoch\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(trainloader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77123b56-9924-46a6-8f26-3cf4d3b93461",
   "metadata": {},
   "source": [
    "`Step 4: Implementing Batch Normalization`\n",
    "\n",
    "Now, we'll modify our neural network model to include batch normalization layers after each hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c32754-d992-4525-9b2b-a55847f629c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNNWithBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNNWithBN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)  # Batch Normalization after the first hidden layer\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)  # Batch Normalization after the second hidden layer\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)  # Flatten the input\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model with Batch Normalization\n",
    "model_with_bn = SimpleNNWithBN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d72784f-fb21-46e2-bc7a-509b4a7f3853",
   "metadata": {},
   "source": [
    "`Step 5: Training the Neural Network with Batch Normalization`\n",
    "\n",
    "Now, we'll train the neural network with batch normalization and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58112f6a-719a-4b54-83be-cc388d6efd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_with_bn = optim.SGD(model_with_bn.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Training loop with batch normalization\n",
    "for epoch in range(5):  # Number of epochs\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer_with_bn.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = model_with_bn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_with_bn.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print loss after each epoch\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {running}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e686ca-3388-4aa7-ab44-fd4fe08bdb85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9a7eb8d-fa8b-4c86-ac8a-16b8d03e5dd9",
   "metadata": {},
   "source": [
    "Q3. `Experimentation and analysis`\n",
    "1. Experiment with different batch sizes and observe the effect on the training dynamics and model performance.\n",
    "2. Discuss the advantages and potential limitations of batch normalization in improving the training of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3739106-f0a4-443b-b36c-4f3dc6c0c338",
   "metadata": {},
   "source": [
    "`1. Experiment with different batch sizes and observe the effect on the training dynamics and model performance:`\n",
    "   Batch size is an important hyperparameter in training neural networks, including those with batch normalization. The batch size determines the number of samples processed before the model's parameters are updated during each training iteration. Larger batch sizes tend to provide more stable updates, but they may require more memory and can result in longer training times.\n",
    "\n",
    "   For the experiment, you can try training the neural network with batch normalization using different batch sizes (e.g., 32, 64, 128) and observe the following:\n",
    "\n",
    "   - Training Time: Larger batch sizes may result in faster convergence, but they might also require more memory and longer computation time per iteration.\n",
    "   - Training Loss: Observe how the training loss changes with different batch sizes. Larger batch sizes might lead to smoother loss curves during training.\n",
    "   - Generalization: Compare the model's performance on the validation set for different batch sizes. Smaller batch sizes may lead to better generalization in some cases.\n",
    "\n",
    "`2. Discuss the advantages and potential limitations of batch normalization in improving the training of neural networks:\n",
    "   Advantages of Batch Normalization:`\n",
    "   - Faster Convergence: Batch normalization normalizes the activations, which helps alleviate the vanishing and exploding gradient problems. This can lead to faster convergence during training.\n",
    "   - Less Sensitive to Initialization: Batch normalization makes neural networks less sensitive to the choice of weight initialization, reducing the need for careful weight initialization techniques.\n",
    "   - Regularization Effect: Batch normalization acts as a form of regularization by adding noise to the activations, reducing the risk of overfitting.\n",
    "   - Higher Learning Rates: With batch normalization, higher learning rates can be used during training without diverging, leading to faster learning.\n",
    "\n",
    "   `Potential Limitations of Batch Normalization:`\n",
    "   - Computation Overhead: Batch normalization introduces additional computations during training, which may slow down the training process, especially for large batch sizes.\n",
    "   - Batch Size Dependency: Batch normalization performance may vary depending on the batch size. Smaller batch sizes might not benefit as much as larger batch sizes.\n",
    "   - Training and Inference Discrepancy: During inference (prediction), the model may not see batches of data, and thus, batch normalization may behave differently during training and inference. Techniques like running statistics or using the entire dataset for normalization during inference can help mitigate this issue.\n",
    "\n",
    "   It is essential to experiment with batch normalization and other techniques on different datasets and architectures to understand their effects fully. Batch normalization is a powerful tool for improving the training of neural networks, but its effectiveness may vary depending on the specific problem and model architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
